
@book{ramsay_functional_2010,
	address = {New York (N.Y.)},
	title = {Functional data analysis},
	isbn = {978-1-4419-2300-4},
	language = {English},
	publisher = {Springer Science+Business Media},
	author = {Ramsay, James O and Silverman, Bernard W},
	year = {2010}
}

@article{malenovsky_sentinels_2012,
	series = {The {Sentinel} {Missions} - {New} {Opportunities} for {Science}},
	title = {Sentinels for science: {Potential} of {Sentinel}-1, -2, and -3 missions for scientific observations of ocean, cryosphere, and land},
	volume = {120},
	issn = {0034-4257},
	shorttitle = {Sentinels for science},
	url = {http://www.sciencedirect.com/science/article/pii/S0034425712000648},
	doi = {10.1016/j.rse.2011.09.026},
	abstract = {The Sentinel-1, -2, and -3 satellite missions can meet various observational needs for spatially explicit physical, biogeophysical, and biological variables of the ocean, cryosphere, and land research activities. The currently known observational requirements were extracted from documents produced by major international scientific projects and programs. The summarized observational needs were then cross-referenced with the capabilities of the planned sensors aboard of the first three Sentinels. A comparative analysis, also incorporating scientific challenges of the ESA Living Planet Programme and the Essential Climate Variables (ECVs), resulted in a preliminary scientific priority assessment of the reviewed environmental variables. Results of these activities, discussed and consolidated in March 2011 at the Sentinels for Science (SEN4SCI) scientific workshop, demonstrate the high potential of the Sentinel-1, -2, and -3 missions for systematic, long-term observations of the Earth system.},
	urldate = {2019-06-17},
	journal = {Remote Sensing of Environment},
	author = {Malenovský, Zbyněk and Rott, Helmut and Cihlar, Josef and Schaepman, Michael E. and García-Santos, Glenda and Fernandes, Richard and Berger, Michael},
	month = may,
	year = {2012},
	pages = {91--101},
	file = {ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/GH4J8PZ3/S0034425712000648.html:text/html}
}

@article{aschbacher_european_2012,
	series = {The {Sentinel} {Missions} - {New} {Opportunities} for {Science}},
	title = {The {European} {Earth} monitoring ({GMES}) programme: {Status} and perspectives},
	volume = {120},
	issn = {0034-4257},
	shorttitle = {The {European} {Earth} monitoring ({GMES}) programme},
	url = {http://www.sciencedirect.com/science/article/pii/S0034425712000612},
	doi = {10.1016/j.rse.2011.08.028},
	abstract = {Global Monitoring for Environment and Security (GMES) is the most ambitious operational Earth Observation programme to date and will provide global, timely and easily accessible information in application domains such as land, marine, atmosphere, emergency response, climate change and security. To accomplish this, the European Union (EU)-led GMES programme comprises three components namely the space component, the in-situ component and the service component. The space component, led by ESA, is in its pre-operational stage, serving users with satellite data acquired by the so called “GMES Contributing Missions” already available today or planned at European, national and international level. It will become operational once the dedicated space infrastructure, comprised by the “Sentinel” missions and their corresponding ground segments are operational. The first of these satellite series will be launched in 2013. The Sentinel missions will provide a unique set of observations utilising different techniques spanning C-band SAR, mid to medium resolution optical and thermal observations with increased spectral resolution, altimeter and dedicated spectrometers for atmospheric chemistry. This data, combined with in-situ data, some assimilated into models, will then be turned into services for monitoring the environment, the climate and for security related issues. The GMES Space Component (GSC) is organised in two overlapping phases: the development phase and the operational phase, the latter planned to start in 2014. The main challenge is now to ensure the programme's long-term sustainability.},
	urldate = {2019-06-17},
	journal = {Remote Sensing of Environment},
	author = {Aschbacher, Josef and Milagro-Pérez, Maria Pilar},
	month = may,
	year = {2012},
	pages = {3--8},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/SMFLBCG2/Aschbacher and Milagro-Pérez - 2012 - The European Earth monitoring (GMES) programme St.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/IVXY3JQM/S0034425712000612.html:text/html}
}

@article{zhou_wang_image_2004,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	issn = {1941-0042},
	shorttitle = {Image quality assessment},
	doi = {10.1109/TIP.2003.819861},
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {{Zhou Wang} and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	keywords = {Algorithms, data compression, Data Interpretation, Statistical, Data mining, Degradation, distorted image, error sensitivity, error visibility, human visual perception, human visual system, Humans, Hypermedia, image coding, image compression, image database, Image Enhancement, Image Interpretation, Computer-Assisted, Image quality, Indexes, Information Storage and Retrieval, JPEG, JPEG2000, Layout, Models, Statistical, Pattern Recognition, Automated, perceptual image quality assessment, Quality assessment, Quality Control, reference image, Reproducibility of Results, Sensitivity and Specificity, Signal Processing, Computer-Assisted, structural information, structural similarity index, Subtraction Technique, Transform coding, visual perception, Visual perception, Visual system},
	pages = {600--612},
	file = {IEEE Xplore Abstract Record:/Users/b7064522/Zotero/storage/52HNADDS/Zhou Wang et al. - 2004 - Image quality assessment from error visibility to.html:text/html;IEEE Xplore Full Text PDF:/Users/b7064522/Zotero/storage/PMPZZFVR/Zhou Wang et al. - 2004 - Image quality assessment from error visibility to.pdf:application/pdf;Wang et al. - 2004 - Image Quality Assessment From Error Visibility to.pdf:/Users/b7064522/Zotero/storage/T43MAJP8/Wang et al. - 2004 - Image Quality Assessment From Error Visibility to.pdf:application/pdf}
}

@article{shang_survey_2013,
	title = {A survey of functional principal component analysis},
	volume = {98},
	issn = {1863-8171},
	url = {https://browzine.com/articles/39384090},
	doi = {10.1007/s10182-013-0213-1},
	number = {2},
	urldate = {2020-01-30},
	journal = {AStA Advances in Statistical Analysis},
	author = {Shang, HanLin},
	month = apr,
	year = {2013},
	keywords = {Dimension reduction, Explanatory analysis, Functional data clustering, Functional data forecasting, Functional data modeling},
	pages = {121--142},
	file = {Springer Full Text PDF:/Users/b7064522/Zotero/storage/CKJ3JFMV/Shang - 2014 - A survey of functional principal component analysi.pdf:application/pdf}
}

@book{hyndman_forecasting_2018,
	title = {Forecasting: principles and practice},
	isbn = {978-0-9875071-1-2},
	shorttitle = {Forecasting},
	abstract = {Forecasting is required in many situations. Stocking an inventory may require forecasts of demand months in advance. Telecommunication routing requires traffic forecasts a few minutes ahead. Whatever the circumstances or time horizons involved, forecasting is an important aid in effective and efficient planning.This textbook provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to use them sensibly.},
	language = {en},
	publisher = {OTexts},
	author = {Hyndman, Rob J. and Athanasopoulos, George},
	month = may,
	year = {2018},
	note = {Google-Books-ID: \_bBhDwAAQBAJ}
}

@article{shang_ftsa_2013,
	title = {ftsa: {An} {R} {Package} for {Analyzing} {Functional} {Time} {Series}},
	volume = {5},
	issn = {2073-4859},
	shorttitle = {ftsa},
	url = {https://journal.r-project.org/archive/2013/RJ-2013-006/index.html},
	doi = {10.32614/RJ-2013-006},
	abstract = {Recent advances in computer recording and storing technology have tremendously increased the presence of functional data, whose graphical representation can be inﬁnite-dimensional curve, image, or shape. When the same functional object is observed over a period of time, such data are known as functional time series. This article makes ﬁrst attempt to describe several techniques (centered around functional principal component analysis) for modeling and forecasting functional time series from a computational aspect, using a readily-available R addon package. These methods are demonstrated using age-speciﬁc Australian fertility rate data from 1921 to 2006, and monthly sea surface temperature data from January 1950 to December 2011.},
	language = {en},
	number = {1},
	urldate = {2020-05-01},
	journal = {The R Journal},
	author = {Shang, Lin, Han},
	year = {2013},
	pages = {64},
	file = {Shang - 2013 - ftsa An R Package for Analyzing Functional Time S.pdf:/Users/b7064522/Zotero/storage/J4ITTXVU/Shang - 2013 - ftsa An R Package for Analyzing Functional Time S.pdf:application/pdf}
}

@article{karhunen_zur_1946,
	title = {Zur {Spektraltheorie} stochastischer {Prozesse}},
	volume = {1},
	journal = {Ann. Acad. Sci. Finnicae, Ser. A},
	author = {Karhunen, K.},
	year = {1946},
	keywords = {imported},
	pages = {34}
}

@article{loeve_fonctions_1946,
	title = {Fonctions aléatoires à décomposition orthogonale exponentielle},
	volume = {84},
	journal = {La Revue Scientifique},
	author = {Loève, Michel},
	year = {1946},
	pages = {159--162}
}

@article{tucker_determination_1958,
	title = {Determination of parameters of a functional relation by factor analysis},
	volume = {23},
	number = {1},
	journal = {Psychometrika},
	author = {Tucker, Ledyard R},
	year = {1958},
	note = {Publisher: Springer},
	pages = {19--23}
}

@article{switzer_minmax_1984,
	title = {Min/{Max} autocorrelation factors for multivariate spatial imaging},
	journal = {Technical Report No.6},
	author = {Switzer, P. and Green, A.A.},
	year = {1984},
	note = {Publisher: Department of Statistics, Stanford University, Stanford, CA},
	pages = {14}
}

@article{wang_functional_2016,
	title = {Functional {Data} {Analysis}},
	volume = {3},
	url = {https://doi.org/10.1146/annurev-statistics-041715-033624},
	doi = {10.1146/annurev-statistics-041715-033624},
	abstract = {With the advance of modern technology, more and more data are being recorded continuously during a time interval or intermittently at several discrete time points. These are both examples of functional data, which has become a commonly encountered type of data. Functional data analysis (FDA) encompasses the statistical methodology for such data. Broadly interpreted, FDA deals with the analysis and theory of data that are in the form of functions. This paper provides an overview of FDA, starting with simple statistical notions such as mean and covariance functions, then covering some core techniques, the most popular of which is functional principal component analysis (FPCA). FPCA is an important dimension reduction tool, and in sparse data situations it can be used to impute functional data that are sparsely observed. Other dimension reduction approaches are also discussed. In addition, we review another core technique, functional linear regression, as well as clustering and classification of functional data. Beyond linear and single- or multiple- index methods, we touch upon a few nonlinear approaches that are promising for certain applications. They include additive and other nonlinear functional regression models and models that feature time warping, manifold learning, and empirical differential equations. The paper concludes with a brief discussion of future directions.},
	number = {1},
	urldate = {2020-05-04},
	journal = {Annual Review of Statistics and Its Application},
	author = {Wang, Jane-Ling and Chiou, Jeng-Min and Müller, Hans-Georg},
	year = {2016},
	note = {\_eprint: https://doi.org/10.1146/annurev-statistics-041715-033624},
	pages = {257--295},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/VMAWPJEU/Wang et al. - 2016 - Functional Data Analysis.pdf:application/pdf}
}

@article{yao_functional_2005,
	title = {Functional {Data} {Analysis} for {Sparse} {Longitudinal} {Data}},
	volume = {100},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/27590579},
	abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the covariance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement. A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle.},
	number = {470},
	urldate = {2020-05-14},
	journal = {Journal of the American Statistical Association},
	author = {Yao, Fang and Müller, Hans-Georg and Wang, Jane-Ling},
	year = {2005},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {577--590},
	file = {Yao et al. - 2005 - Functional Data Analysis for Sparse Longitudinal D.pdf:/Users/b7064522/Zotero/storage/SW96SQ9Y/Yao et al. - 2005 - Functional Data Analysis for Sparse Longitudinal D.pdf:application/pdf}
}

@book{williams_gaussian_2006,
	title = {Gaussian processes for machine learning},
	volume = {2},
	number = {3},
	publisher = {MIT press Cambridge, MA},
	author = {Williams, Christopher KI and Rasmussen, Carl Edward},
	year = {2006}
}

@article{hooker_maximal_2016,
	title = {Maximal autocorrelation functions in functional data analysis},
	volume = {26},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-015-9582-5},
	doi = {10.1007/s11222-015-9582-5},
	abstract = {This paper proposes a new factor rotation for the context of functional principal components analysis. This rotation seeks to re-express a functional subspace in terms of directions of decreasing smoothness as represented by a generalized smoothing metric. The rotation can be implemented simply and we show on two examples that this rotation can improve the interpretability of the leading components.},
	language = {en},
	number = {5},
	urldate = {2020-06-04},
	journal = {Statistics and Computing},
	author = {Hooker, Giles and Roberts, Steven},
	month = sep,
	year = {2016},
	pages = {945--950},
	file = {Hooker and Roberts - 2016 - Maximal autocorrelation functions in functional da.pdf:/Users/b7064522/Zotero/storage/TBHAKW34/Hooker and Roberts - 2016 - Maximal autocorrelation functions in functional da.pdf:application/pdf}
}

@article{liu_functional_2017,
	title = {Functional principal component analysis of spatially correlated data},
	volume = {27},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-016-9708-4},
	doi = {10.1007/s11222-016-9708-4},
	abstract = {This paper focuses on the analysis of spatially correlated functional data. We propose a parametric model for spatial correlation and the between-curve correlation is modeled by correlating functional principal component scores of the functional data. Additionally, in the sparse observation framework, we propose a novel approach of spatial principal analysis by conditional expectation to explicitly estimate spatial correlations and reconstruct individual curves. Assuming spatial stationarity, empirical spatial correlations are calculated as the ratio of eigenvalues of the smoothed covariance surface Cov\$\$(X\_i(s),X\_i(t))\$\$(Xi(s),Xi(t))and cross-covariance surface Cov\$\$(X\_i(s), X\_j(t))\$\$(Xi(s),Xj(t))at locations indexed by i and j. Then a anisotropy Matérn spatial correlation model is fitted to empirical correlations. Finally, principal component scores are estimated to reconstruct the sparsely observed curves. This framework can naturally accommodate arbitrary covariance structures, but there is an enormous reduction in computation if one can assume the separability of temporal and spatial components. We demonstrate the consistency of our estimates and propose hypothesis tests to examine the separability as well as the isotropy effect of spatial correlation. Using simulation studies, we show that these methods have some clear advantages over existing methods of curve reconstruction and estimation of model parameters.},
	language = {en},
	number = {6},
	urldate = {2020-06-19},
	journal = {Statistics and Computing},
	author = {Liu, Chong and Ray, Surajit and Hooker, Giles},
	month = nov,
	year = {2017},
	pages = {1639--1654},
	file = {Liu et al. - 2017 - Functional principal component analysis of spatial.pdf:/Users/b7064522/Zotero/storage/IBUFMS9S/Liu et al. - 2017 - Functional principal component analysis of spatial.pdf:application/pdf;Springer Full Text PDF:/Users/b7064522/Zotero/storage/T3VPCMSN/Liu et al. - 2017 - Functional principal component analysis of spatial.pdf:application/pdf}
}

@book{piegl_nurbs_1997,
	address = {Berlin ; New York},
	edition = {2nd ed},
	series = {Monographs in visual communications},
	title = {The {NURBS} book},
	isbn = {978-3-540-61545-3},
	language = {en},
	number = {1},
	publisher = {Springer},
	author = {Piegl, Les A. and Tiller, Wayne},
	year = {1997},
	file = {Piegl and Tiller - 1997 - The NURBS book.pdf:/Users/b7064522/Zotero/storage/W7ACXDZC/Piegl and Tiller - 1997 - The NURBS book.pdf:application/pdf}
}

@article{wood_low-rank_2006,
	title = {Low-rank scale-invariant tensor product smooths for generalized additive mixed models},
	volume = {62},
	issn = {0006-341X},
	doi = {10.1111/j.1541-0420.2006.00574.x},
	abstract = {A general method for constructing low-rank tensor product smooths for use as components of generalized additive models or generalized additive mixed models is presented. A penalized regression approach is adopted in which tensor product smooths of several variables are constructed from smooths of each variable separately, these "marginal" smooths being represented using a low-rank basis with an associated quadratic wiggliness penalty. The smooths offer several advantages: (i) they have one wiggliness penalty per covariate and are hence invariant to linear rescaling of covariates, making them useful when there is no "natural" way to scale covariates relative to each other; (ii) they have a useful tuneable range of smoothness, unlike single-penalty tensor product smooths that are scale invariant; (iii) the relatively low rank of the smooths means that they are computationally efficient; (iv) the penalties on the smooths are easily interpretable in terms of function shape; (v) the smooths can be generated completely automatically from any marginal smoothing bases and associated quadratic penalties, giving the modeler considerable flexibility to choose the basis penalty combination most appropriate to each modeling task; and (vi) the smooths can easily be written as components of a standard linear or generalized linear mixed model, allowing them to be used as components of the rich family of such models implemented in standard software, and to take advantage of the efficient and stable computational methods that have been developed for such models. A small simulation study shows that the methods can compare favorably with recently developed smoothing spline ANOVA methods.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Wood, Simon N.},
	month = dec,
	year = {2006},
	pmid = {17156276},
	pages = {1025--1036},
	file = {Accepted Version:/Users/b7064522/Zotero/storage/UMW4ICHL/Wood - 2006 - Low-rank scale-invariant tensor product smooths fo.pdf:application/pdf}
}

@book{abramowitz_handbook_2013,
	address = {New York, NY},
	series = {Dover books on mathematics},
	title = {Handbook of mathematical functions: with formulas, graphs, and mathematical tables},
	language = {eng},
	number = {1},
	publisher = {Dover Publ},
	editor = {Abramowitz, Milton and Stegun, Irene A.},
	year = {2013}
}

@article{lukas_robust_2006,
	title = {Robust generalized cross-validation for choosing the regularization parameter},
	volume = {22},
	issn = {0266-5611},
	url = {https://doi.org/10.1088/0266-5611/22/5/021},
	doi = {10.1088/0266-5611/22/5/021},
	abstract = {Let fλ be the regularized solution for the problem of estimating a function or vector f0 from noisy data yi = Lif0 + εi, i = 1, …, n, where Li are linear functionals. A prominent method for the selection of the crucial regularization parameter λ is generalized cross-validation (GCV). It is known that GCV has good asymptotic properties as n → ∞ but it may not be reliable for small or medium sized n, sometimes giving an estimate that is far too small. We propose a new robust GCV method (RGCV) which chooses λ to be the minimizer of γV(λ) + (1 − γ)F(λ), where V(λ) is the GCV function, F(λ) is an approximate average measure of the influence of each data point on fλ and γ ∊ (0, 1) is a robustness parameter. We show that for any n, RGCV is less likely than GCV to choose a very small value of λ, resulting in a more robust method. We also show that RGCV has good asymptotic properties as n → ∞ for general linear operator equations with uncorrelated errors. The function EF(λ) approximates the risk ER(λ) for values of λ that are asymptotically a bit smaller than the minimizer of ER(λ) (where V(λ) may not approximate well). The ‘expected’ RGCV estimate is asymptotically optimal as n → ∞ with respect to the ‘robust risk’ γER(λ) + (1 − γ)v(λ), where v(λ) is the variance component of the risk, and it has the optimal decay rate with respect to ER(λ) and stronger error criteria. The GCV and RGCV methods are compared in numerical simulations for the problem of estimating the second derivative from noisy data. The results for RGCV with n = 51 are consistent with the asymptotic results, and, for a large range of γ values, RGCV is more reliable and accurate than GCV.},
	language = {en},
	number = {5},
	urldate = {2021-01-26},
	journal = {Inverse Problems},
	author = {Lukas, Mark A.},
	month = sep,
	year = {2006},
	note = {Publisher: IOP Publishing},
	pages = {1883--1902},
	file = {Full Text:/Users/b7064522/Zotero/storage/KB6URWZK/Lukas - 2006 - Robust generalized cross-validation for choosing t.pdf:application/pdf}
}

@book{jolliffe_principal_2002,
	address = {New York},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {Principal component analysis},
	isbn = {978-0-387-95442-4},
	publisher = {Springer},
	author = {Jolliffe, I. T.},
	year = {2002}
}
