{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname('.'), '..','src'))\n",
    "\n",
    "import numpy as np\n",
    "from basis import Bspline\n",
    "from utils import GP, Matern\n",
    "from fda import fpca, mafr\n",
    "import tqdm\n",
    "\n",
    "np.random.seed(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Simulated Dataset\n",
    "\n",
    "In this notebook we forecast our simulated data set using a functional time series style approach [1]. We consider a number of functional decompositions to facilitate this approach. In particular we consider the following decompositions:\n",
    "\n",
    "* Functional Principal Component Analysis [2]\n",
    "* Maximum Autocorrelation Factor Rotations [3]\n",
    "* Regularised Functional Principal Component Analysis \n",
    "* Regularised Maximum Autocorrelation Factor Rotations\n",
    "\n",
    "[1]: H. Shang Lin, ‘ftsa: An R Package for Analyzing Functional Time Series’, The R Journal, vol. 5, no. 1, p. 64, 2013, doi: 10.32614/RJ-2013-006.\n",
    "\n",
    "[2]: J. O. Ramsay and B. W. Silverman, Functional data analysis. New York (N.Y.): Springer Science+Business Media, 2010.\n",
    "\n",
    "[3]: G. Hooker and S. Roberts, ‘Maximal autocorrelation functions in functional data analysis’, Stat Comput, vol. 26, no. 5, pp. 945–950, Sep. 2016, doi: 10.1007/s11222-015-9582-5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "The general methodology for forecasting time series of functional data using a functional decomposition is given in [1]. An overview however is below, and can be generally broken into the following steps:\n",
    "\n",
    "1. Obtain a functional decomposition (FPCA for example) of the time series of funcitonal observations.\n",
    "2. This gives a collection of eigenfunctions and related scores.\n",
    "3. Treat each series as independent and forecast using univariate time series methods. \n",
    "4. Reconstruct the functional variables for future time points by reconstructing from estimated eigenfunctions and the **forecast** for the corresponding score series for the future time points. \n",
    "\n",
    "In our work below we complete such steps 1-4 for each simulation generated with the various noise scenarios as described in [here]. We consider 1 to 25 steps ahead forecast for each simulated data set, with the first 90 time points being considered as a training set. We do this in a procedural manner with adding an observation and repeating the forecast with an additional observation for each simulation, allowing us to obtain an average over the data set various steps ahead forecasts. \n",
    "\n",
    "We complete this process for each simulation generated [here] and save the results for each noise type and decomposition type in a seperate `npz` binary file. \n",
    "\n",
    "[here]: ./data_generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In the following codeblock we setup the domain parameters, a basis system for our functional decomposition and the various inner products needed for the decomposition methodology. These are constant and unaffected by the noise processes so we can calculate them just once for the whole notebook. In particular we set our basis to have $25$ basis functions in each dimension of our spatial functional data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Domain parameters\n",
    "S1, S2, T = 128, 128, 128\n",
    "t = np.linspace(0,1,T)\n",
    "\n",
    "## Basis system \n",
    "bs = Bspline((-1,1), 25, 4)\n",
    "B = np.kron(bs(np.linspace(-1,1,S1)), bs(np.linspace(-1,1,S1)))\n",
    "J = np.kron(np.eye(bs.K), bs.penalty(0)) + np.kron(bs.penalty(0), np.eye(bs.K))\n",
    "\n",
    "## Penalties for regularisation and mafr. \n",
    "NDERIV = 2\n",
    "P = np.kron(np.eye(bs.K), bs.penalty(NDERIV)) + np.kron(bs.penalty(NDERIV), np.eye(bs.K))\n",
    "LOG_LAMBDA = -5.0\n",
    "\n",
    "## Step sizes, training data set, and components to use.\n",
    "STEPS = 25\n",
    "N_INIT = 90\n",
    "N_COMP = 5\n",
    "\n",
    "## Constant simulated data (nsimulations X datasets)\n",
    "SIM_PATH = '../data/simulated.npz'\n",
    "LN_PATH = '../data/simulated_ln.npz'\n",
    "HN_PATH = '../data/simulated_hn.npz'\n",
    "SN_PATH = '../data/simulated_sn.npz'\n",
    "data = np.load(SIM_PATH)\n",
    "C_arr = data['C']\n",
    "Y = np.einsum(\"ij, kjl->kil\", data['PHI'], C_arr).swapaxes(-1,1)\n",
    "\n",
    "NSIM = len(C_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "The following codeblock runs the forecasting using an each decomposition for each of the low noise, high noise and structure noise scenario. We complete this for 100 simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [34:53<00:00, 20.93s/it]\n"
     ]
    }
   ],
   "source": [
    "noise_labels = ['ln', 'hn', 'sn']\n",
    "models = ['fpca', 'mafr','reg-fpca','reg-mafr']\n",
    "noise_paths = [LN_PATH, HN_PATH, SN_PATH]\n",
    "results={m+'_'+n:np.zeros((NSIM, np.max(STEPS))) for m in models for n in noise_labels}\n",
    "for i in tqdm.tqdm(np.arange(NSIM)):\n",
    "    for label, path in zip(noise_labels, noise_paths):\n",
    "        noise = np.load(path)['sim'][i]\n",
    "        Y_e = Y[i] + noise\n",
    "        for model in models:\n",
    "            ll = LOG_LAMBDA if model.startswith('reg') else -14.0\n",
    "            mafr_ind = True if 'mafr' in model else False\n",
    "            Y_train = Y_e[:N_INIT]\n",
    "            x_ob = t[:N_INIT]\n",
    "            x_new = t[N_INIT:(N_INIT+STEPS)]\n",
    "            Cbar, zeta, scores, w = fpca(Y_train.T, B, P, ll, NDERIV, J, N_COMP)\n",
    "            if mafr_ind:\n",
    "                zeta, scores, U = mafr(zeta, scores, P)\n",
    "            mu_fors = []\n",
    "            V_fors = []\n",
    "            for score in scores.T:\n",
    "                gp = GP(Matern(nu=2.5, rho=1.0, sigma=1.0))\n",
    "                gp.fit(x_ob, score.T, bounds=[(-6,2), (-6,2), (-6, 2)], n_init=100)\n",
    "                mu_for, V_for = gp.posterior(x_new)\n",
    "                mu_fors.append(mu_for)\n",
    "                V_fors.append(np.diag(V_for))\n",
    "            recon = np.matmul(np.matmul(B, zeta), mu_fors) + np.matmul(B, Cbar)[:, np.newaxis]\n",
    "            results[model+'_'+label][i,: ] = np.sqrt(np.mean((Y[i][N_INIT:(N_INIT+STEPS)].T-recon)**2, axis=0))\n",
    "for key in results.keys():\n",
    "    np.savez('../results/simulated_'+key+'.npz', results=results[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dftsm",
   "language": "python",
   "name": "dftsm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
